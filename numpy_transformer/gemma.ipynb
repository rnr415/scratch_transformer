{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/Users/rajeshnrao/projects/git_repos/transformers/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.43.0.dev0_rnr415\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma vocab len - 256000\n",
      "Gemma vocab - ['▁esperaba', 'phins', '▁daftar', '▁botany', 'cleos', 'ͼ', '℮', '▁möjlighet', '录音', 'teiro']\n",
      "126\n",
      "['<bos>', 'Original', '▁S', 'ceptre', '▁', '8', '1', '4', '2', '0', '2', '6', '6', '7', '0', '0', '0', '3', 'C', '▁TV', '▁Remote', '▁Control', '▁E', '1', '6', '5', 'BV', '(', 'WV', ')-', 'SS', '▁E', '1', '6', '8', 'BV', '(', 'WV', ')-', 'SSE', '2', '0', '5', 'BV', '-', 'SM', 'QC', 'CE', '3', '2', '5', 'BV', '-', 'SR', '▁E', '3', '2', '8', 'BV', '(', 'WV', ')-', 'SR', 'X', '3', '2', '5', 'BV', '-', 'F', 'SR', '▁X', '4', '0', '5', 'BV', '-', 'F', 'SR', 'X', '5', '0', '5', 'BV', '-', 'F', 'SR', 'U', '5', '0', '5', 'CV', '-', 'UM', 'R', '▁U', '5', '0', '8', 'CV', '-', 'UM', 'KR', 'U', '5', '5', '0', 'CV', '-', 'UM', '0', '8', 'RU', '6', '5', '0', 'CV', '-', 'UM', 'RU', '7', '5', '0', 'CV', '-', 'UM']\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Original Sceptre 8142026670003C TV Remote Control E165BV(WV)-SS E168BV(WV)-SSE205BV-SMQCCE325BV-SR E328BV(WV)-SRX325BV-FSR X405BV-FSRX505BV-FSRU505CV-UMR U508CV-UMKRU550CV-UM08RU650CV-UMRU750CV-UM\"\n",
    "text_2 = \"Metene Pulse Oximeter Fingertip with Batteries and Lanyard, OLED Blood Oxygen Saturation Monitor, 20E\"\n",
    "text_tokens = tokenizer_gemma(text_1)\n",
    "\n",
    "\n",
    "token_text = tokenizer_gemma.convert_ids_to_tokens(text_tokens.input_ids)\n",
    "vocab_gemma = list(tokenizer_gemma.vocab)\n",
    "\n",
    "print(f\"Gemma vocab len - {len(vocab_gemma)}\")\n",
    "print(f\"Gemma vocab - {vocab_gemma[-10:]}\")\n",
    "\n",
    "print(len(text_tokens.input_ids))\n",
    "print(token_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Original', '▁S', 'cept', 're', '▁81', '420', '26', '670', '003', 'C', '▁TV', '▁Remote', '▁Control', '▁E', '165', 'BV', '(', 'W', 'V', ')', '-', 'SS', '▁E', '168', 'BV', '(', 'W', 'V', ')', '-', 'SSE', '205', 'BV', '-', 'SM', 'Q', 'C', 'CE', '3', '25', 'BV', '-', 'SR', '▁E', '3', '28', 'BV', '(', 'W', 'V', ')', '-', 'SR', 'X', '3', '25', 'BV', '-', 'F', 'SR', '▁X', '40', '5', 'BV', '-', 'F', 'SR', 'X', '50', '5', 'BV', '-', 'FS', 'RU', '50', '5', 'CV', '-', 'UM', 'R', '▁U', '50', '8', 'CV', '-', 'UM', 'K', 'RU', '550', 'CV', '-', 'UM', '08', 'RU', '650', 'CV', '-', 'UM', 'RU', '750', 'CV', '-', 'UM', '</s>']\n",
      "metene pulse oximeter fingertip with batteries and lanyard, oled blood oxygen saturation monitor, 20e\n",
      "original sceptre 8142026670003c tv remote control e165bv(wv)-ss e168bv(wv)-sse205bv-smqcce325bv-sr e328bv(wv)-srx325bv-fsr x405bv-fsrx505bv-fsru505cv-umr u508cv-umkru550cv-um08ru650cv-umru750cv-um\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bge = AutoTokenizer.from_pretrained('BAAI/bge-m3')\n",
    "bge_tokens = tokenizer_bge.encode(text_1)\n",
    "\n",
    "bge_token_text = tokenizer_bge.convert_ids_to_tokens(bge_tokens)\n",
    "print(bge_token_text)\n",
    "print(text_2.lower())\n",
    "print(text_1.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original  Sceptre   814 202 667 000 3 C  TV  Remote  Control  E 165 BV (WV )- SS  E 168 BV (WV )- SSE 205 BV -SMQCCE 325 BV -SR  E 328 BV (WV )- SRX 325 BV -FSR  X 405 BV -FSRX 505 BV -FSRU 505 CV -UMR  U 508 CV -UMKRU 550 CV -UM 08 RU 650 CV -UMRU 750 CV -UM\n",
      "['<s>', '▁Original', '▁S', 'cept', 're', '▁8', '14', '▁202', '▁', '667', '▁000', '▁3', '▁C', '▁TV', '▁Remote', '▁Control', '▁E', '▁165', '▁', 'BV', '▁(', 'W', 'V', '▁)', '-', '▁SS', '▁E', '▁168', '▁', 'BV', '▁(', 'W', 'V', '▁)', '-', '▁', 'SSE', '▁205', '▁', 'BV', '▁-', 'SM', 'Q', 'C', 'CE', '▁325', '▁', 'BV', '▁-', 'SR', '▁E', '▁3', '28', '▁', 'BV', '▁(', 'W', 'V', '▁)', '-', '▁SR', 'X', '▁325', '▁', 'BV', '▁-', 'F', 'SR', '▁X', '▁40', '5', '▁', 'BV', '▁-', 'F', 'SR', 'X', '▁50', '5', '▁', 'BV', '▁-', 'FS', 'RU', '▁50', '5', '▁CV', '▁-', 'UM', 'R', '▁U', '▁50', '8', '▁CV', '▁-', 'UM', 'K', 'RU', '▁550', '▁CV', '▁-', 'UM', '▁08', '▁RU', '▁650', '▁CV', '▁-', 'UM', 'RU', '▁750', '▁CV', '▁-', 'UM', '</s>']\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "# pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "pat_str = \"|\".join(\n",
    "    [\n",
    "        r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?\"\"\",\n",
    "        r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?\"\"\",\n",
    "        r\"\"\"\\p{N}{1,3}\"\"\",\n",
    "        r\"\"\" ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*\"\"\",\n",
    "        r\"\"\"\\s*[\\r\\n]+\"\"\",\n",
    "        r\"\"\"\\s+(?!\\S)\"\"\",\n",
    "        r\"\"\"\\s+\"\"\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "pat = re.compile(pat_str)\n",
    "\n",
    "re_pattern = re.findall(pat, text_1)\n",
    "text_1_processed = ' '.join(re_pattern)\n",
    "bge_tokens = tokenizer_bge.encode(text_1_processed)\n",
    "\n",
    "bge_token_text = tokenizer_bge.convert_ids_to_tokens(bge_tokens)\n",
    "print(text_1_processed)\n",
    "print(bge_token_text)\n",
    "print(len(bge_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.43.0.dev0_rnr415\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [20556, 311, 984, 260, 807, 16, 19, 17, 15, 17, 21, 21, 22, 15, 15, 15, 18, 34, 3195, 21520, 6779, 412, 16, 21, 20, 33, 53, 7, 54, 53, 13219, 5432, 412, 16, 21, 23, 33, 53, 7, 54, 53, 13219, 50, 5188, 17, 15, 20, 33, 53, 12, 12310, 48, 4093, 36, 18, 17, 20, 33, 53, 12, 12562, 412, 18, 17, 23, 33, 53, 7, 54, 53, 13219, 12562, 55, 18, 17, 20, 33, 53, 12, 10652, 49, 1395, 19, 15, 20, 33, 53, 12, 10652, 49, 55, 20, 15, 20, 33, 53, 12, 10652, 49, 52, 20, 15, 20, 33538, 12, 5883, 49, 471, 20, 15, 23, 33538, 12, 5883, 30758, 52, 20, 20, 15, 33538, 12, 5883, 15, 23, 49, 52, 21, 20, 15, 33538, 12, 5883, 49, 52, 22, 20, 15, 33538, 12, 5883], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Original Sceptre 8142026670003C TV Remote Control E165BV(WV)-SS E168BV(WV)-SSE205BV-SMQCCE325BV-SR E328BV(WV)-SRX325BV-FSR X405BV-FSRX505BV-FSRU505CV-UMR U508CV-UMKRU550CV-UM08RU650CV-UMRU750CV-UM\n",
      "['Original', 'ĠS', 'cept', 're', 'Ġ8', '1', '4', '2', '0', '2', '6', '6', '7', '0', '0', '0', '3', 'C', 'ĠTV', 'ĠRemote', 'ĠControl', 'ĠE', '1', '6', '5', 'B', 'V', '(', 'W', 'V', ')-', 'SS', 'ĠE', '1', '6', '8', 'B', 'V', '(', 'W', 'V', ')-', 'S', 'SE', '2', '0', '5', 'B', 'V', '-', 'SM', 'Q', 'CC', 'E', '3', '2', '5', 'B', 'V', '-', 'SR', 'ĠE', '3', '2', '8', 'B', 'V', '(', 'W', 'V', ')-', 'SR', 'X', '3', '2', '5', 'B', 'V', '-', 'FS', 'R', 'ĠX', '4', '0', '5', 'B', 'V', '-', 'FS', 'R', 'X', '5', '0', '5', 'B', 'V', '-', 'FS', 'R', 'U', '5', '0', '5', 'CV', '-', 'UM', 'R', 'ĠU', '5', '0', '8', 'CV', '-', 'UM', 'KR', 'U', '5', '5', '0', 'CV', '-', 'UM', '0', '8', 'R', 'U', '6', '5', '0', 'CV', '-', 'UM', 'R', 'U', '7', '5', '0', 'CV', '-', 'UM']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "#tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "## change transformers/models/gpt2/tokenization_gpt2.py:L167 \n",
    "## from - self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\") \n",
    "## to - self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "gpt2_tokens = tokenizer_gpt2(text_1)\n",
    "\n",
    "print(gpt2_tokens)\n",
    "gpt2_token_text = [tokenizer_gpt2._convert_id_to_token(x) for x in gpt2_tokens.input_ids]\n",
    "print(text_1)\n",
    "print(gpt2_token_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sceptre 8142026670003C TV Remote Control E165BV(WV)-SS E168BV(WV)-SSE205BV-SMQCCE325BV-SR E328BV(WV)-SRX325BV-FSR X405BV-FSRX505BV-FSRU505CV-UMR U508CV-UMKRU550CV-UM08RU650CV-UMRU750CV-UM\n",
      "27\n",
      "['Met', 'ene', ' Pulse', ' Ox', 'imeter', ' F', 'ing', 'ert', 'ip', ' with', ' Batter', 'ies', ' and', ' L', 'any', 'ard', ',', ' OLED', ' Blood', ' Oxygen', ' Sat', 'uration', ' Monitor', ',', ' ', '20', 'E']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "token_ids = enc.encode(text_2)\n",
    "\n",
    "print(text_1)\n",
    "\n",
    "print(len(token_ids))\n",
    "print([enc.decode([x]) for x in token_ids])\n",
    "\n",
    "# print(enc.decode(enc.encode(text_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d98b690e0f4de1b7ff4d97791c1fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c16c5bc7b63412aa6e1d9b14a5a6506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5a1b350c154096aa32037f4f764524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392928e42cfb43e0b0508165e9ca664e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def updater():\n",
      "    \"\"\"Update the current installation.\n",
      "\n",
      "    git clones the latest version and merges it with the current directory.\n",
      "    \"\"\"\n",
      "    print('%s Checking for updates' % run)\n",
      "    # Changes must be separated by ;\n",
      "    changes = '''major bug fixes;removed ninja mode;dropped python < 3.2 support;fixed unicode output;proxy support;more intels'''\n",
      "    latest_commit = requester('https://raw.githubusercontent.com/s0md3v/Photon/master/core/updater.py', host='raw.githubusercontent.com')\n",
      "    # Just a hack to see if a new version is available\n",
      "    if changes not in latest_commit:\n",
      "        changelog = re.search(r\"changes = '''(.*?)'''\", latest_commit)\n",
      "        # Splitting the changes to form a list\n",
      "        changelog = changelog.group(1).split(';')\n",
      "        print('%s A new version of Photon is available.' % good)\n",
      "        print('%s Changes:' % info)\n",
      "        for change in changelog: # print changes\n",
      "            print('%s>%s %s' % (green, end, change))\n",
      "\n",
      "        current_path = os.getcwd().split('/') # if you know it, you know it\n",
      "        folder = current_path[-1] # current directory name\n",
      "        path = '/'.join(current_path) # current directory path\n",
      "        choice = input('%s Would you like to update? [Y/n] ' % que).lower()\n",
      "\n",
      "        if choice != 'n':\n",
      "            print('%s Updating Photon' % run)\n",
      "            os.system('git clone --quiet https://github.com/s0md3v/Photon %s'\n",
      "                      % (folder))\n",
      "            os.system('cp -r %s/%s/* %s && rm -r %s/%s/ 2>/dev/null'\n",
      "                      % (path, folder, path, path, folder))\n",
      "            print('%s Update successful!' % good)\n",
      "    else:\n",
      "        print('%s Photon is up to date!' % good)\n"
     ]
    }
   ],
   "source": [
    "# from datasets import Dataset\n",
    "\n",
    "# Dataset({\n",
    "    # features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', \n",
    "      # 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', \n",
    "      # 'func_code_url'\n",
    "    # ],\n",
    "    # num_rows: 412178\n",
    "# })\n",
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0855cef69e4b5595676506a90ffcf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0325f30301945f7a5726aa27ff7af40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a27879302e84a3e8dcfadce94a497bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec80317ff3e465187a4188bca9905d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed5bece9a7246e7a8105c4bd7238bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_new = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "tokens_new = tokenizer_new.tokenize(example)\n",
    "print(tokens_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
