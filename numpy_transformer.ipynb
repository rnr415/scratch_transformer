{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Code from Numpy\n",
        "\n",
        "Write a Transformer code using only Numpy"
      ],
      "metadata": {
        "id": "hP67rxjPQJMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer code consists of multiple blocks\n",
        "\n",
        "- Self Attention Block\n",
        "This take the embedding of the input tokens as input and generates embedding of tokens with Self Attention"
      ],
      "metadata": {
        "id": "YM6hPSHEQikx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "input_str = \"this is a test string\"\n",
        "input_tokens = model.tokenize(input_str)\n",
        "```\n",
        "*input_tokens* will have the list of integer that has the index\n",
        "\n",
        "For example\n",
        "```\n",
        "input_tokens = [342, 4230, 3249, 4509, 149, 4, 0]\n",
        "```\n",
        "\n",
        "The *input_tokens* will be replaced by list of embeddings where we replace each integer with the corresponding vector for that token\n",
        "Now we have:\n",
        "\n",
        "```\n",
        "input_embeddings = [emb_vec_1, emb_vec_2,..... ]\n",
        "input_emb_matrix = input_embeddings.reshape()\n",
        "```\n",
        "\n",
        "The input to the Transformer block is the embeddings.\n",
        "\n",
        "$$attn = softmax \\left(\\frac{K â‹… Q^T}{\\sqrt{d}} \\right)$$\n",
        "\n",
        "where:\n",
        "\n",
        "$K$, $Q$ and $V$ are outputs after input embeddings are linear transformation using matrices $M_K$, $M_Q$ and $M_V$ respectively.\n",
        "\n",
        "```\n",
        "K_emb = numpy.matmul(matrix_M_k . input_emb_matrix)\n",
        "```\n",
        "Lets consider the dimension of this multiplication\n",
        "\n",
        "Dim(input_emb_matrix) = [num_token_in_seq x vec_size]  \n",
        "Dim(matrix_M_k) = [num_token_in_seq x num_token_in_seq]\n",
        "Dim(K_emb) = [num_token_in_seq x vec_size]\n",
        "\n",
        "Similarly the  \n",
        "Dim(matrix_M_q) = [num_token_in_seq x num_token_in_seq]  \n",
        "Dim(matrix_M_v) = [num_token_in_seq x num_token_in_seq]\n",
        "\n",
        "But the number of sequence keeps changing. We need to have a maximum number of tokens permitted.  \n",
        "What is the maximum size of *num_token_in_seq*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OkpuIJAZ1FxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hJNq-5v81Fm6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t7-aiTH-QI2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FCoj0SBeQaBc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ufmp1ibPtQT"
      },
      "outputs": [],
      "source": []
    }
  ]
}